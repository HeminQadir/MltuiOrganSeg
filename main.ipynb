{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    ")\n",
    "\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai import data\n",
    "from monai.data import decollate_batch\n",
    "from functools import partial\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
    "This allows you to save results and reuse downloads.  \n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.data_loader import get_loader\n",
    "\n",
    "data_dir = \"/media/jacobo/NewDrive/Hemin_Collection/BraTS2021\"\n",
    "json_list = \"/media/jacobo/NewDrive/Hemin_Collection/BraTS2021/brats21_folds.json\"\n",
    "roi = (128, 128, 128)\n",
    "batch_size = 1\n",
    "sw_batch_size = 4\n",
    "fold = 1\n",
    "infer_overlap = 0.5\n",
    "max_epochs = 3\n",
    "val_every = 1\n",
    "train_loader, val_loader = get_loader(batch_size, data_dir, json_list, fold, roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set dataset root directory and hyper-parameters\n",
    "\n",
    "The following hyper-parameters are set for the purpose of this tutorial. However, additional changes, as described below, maybe beneficial. \n",
    "\n",
    "If GPU memory is not sufficient, reduce sw_batch_size to 2 or batch_size to 1. \n",
    "\n",
    "Decrease val_every (validation frequency) to 1 for obtaining more accurate checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_add = os.path.join(data_dir, \"/media/jacobo/NewDrive/Hemin_Collection/BraTS2021/TrainingData/BraTS2021_00006/BraTS2021_00006_flair.nii.gz\")\n",
    "label_add = os.path.join(data_dir, \"/media/jacobo/NewDrive/Hemin_Collection/BraTS2021/TrainingData/BraTS2021_00006/BraTS2021_00006_seg.nii.gz\")\n",
    "img = nib.load(img_add).get_fdata()\n",
    "label = nib.load(label_add).get_fdata()\n",
    "print(f\"image shape: {img.shape}, label shape: {label.shape}\")\n",
    "plt.figure(\"image\", (18, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(img[:, :, 78], cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:, :, 78])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Swin UNETR model\n",
    "\n",
    "In this scetion, we create Swin UNETR model for the 3-class brain tumor semantic segmentation.\n",
    "\n",
    "We use a feature size of 48. We also use gradient checkpointing (use_checkpoint) for more memory-efficient training. However, use_checkpoint for faster training if enough GPU memory is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SwinUNETR(\n",
    "    img_size=roi,\n",
    "    in_channels=4,\n",
    "    out_channels=3,\n",
    "    feature_size=48,\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    dropout_path_rate=0.0,\n",
    "    use_checkpoint=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "dice_loss = DiceLoss(to_onehot_y=False, sigmoid=True)\n",
    "post_sigmoid = Activations(sigmoid=True)\n",
    "post_pred = AsDiscrete(argmax=False, threshold=0.5)\n",
    "dice_acc = DiceMetric(include_background=True, reduction=MetricReduction.MEAN_BATCH, get_not_nans=True)\n",
    "model_inferer = partial(\n",
    "    sliding_window_inference,\n",
    "    roi_size=[roi[0], roi[1], roi[2]],\n",
    "    sw_batch_size=sw_batch_size,\n",
    "    predictor=model,\n",
    "    overlap=infer_overlap,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train and Validation Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.helper import AverageMeter\n",
    "\n",
    "def train_epoch(model, loader, optimizer, epoch, loss_func):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    run_loss = AverageMeter()\n",
    "    for idx, batch_data in enumerate(loader):\n",
    "        data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "        logits = model(data)\n",
    "        loss = loss_func(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        run_loss.update(loss.item(), n=batch_size)\n",
    "        print(\n",
    "            \"Epoch {}/{} {}/{}\".format(epoch, max_epochs, idx, len(loader)),\n",
    "            \"loss: {:.4f}\".format(run_loss.avg),\n",
    "            \"time {:.2f}s\".format(time.time() - start_time),\n",
    "        )\n",
    "        start_time = time.time()\n",
    "    return run_loss.avg\n",
    "\n",
    "\n",
    "def val_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    epoch,\n",
    "    acc_func,\n",
    "    model_inferer=None,\n",
    "    post_sigmoid=None,\n",
    "    post_pred=None,\n",
    "):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    run_acc = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch_data in enumerate(loader):\n",
    "            data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "            logits = model_inferer(data)\n",
    "            val_labels_list = decollate_batch(target)\n",
    "            val_outputs_list = decollate_batch(logits)\n",
    "            val_output_convert = [post_pred(post_sigmoid(val_pred_tensor)) for val_pred_tensor in val_outputs_list]\n",
    "            acc_func.reset()\n",
    "            acc_func(y_pred=val_output_convert, y=val_labels_list)\n",
    "            acc, not_nans = acc_func.aggregate()\n",
    "            run_acc.update(acc.cpu().numpy(), n=not_nans.cpu().numpy())\n",
    "            dice_tc = run_acc.avg[0]\n",
    "            dice_wt = run_acc.avg[1]\n",
    "            dice_et = run_acc.avg[2]\n",
    "            print(\n",
    "                \"Val {}/{} {}/{}\".format(epoch, max_epochs, idx, len(loader)),\n",
    "                \", dice_tc:\",\n",
    "                dice_tc,\n",
    "                \", dice_wt:\",\n",
    "                dice_wt,\n",
    "                \", dice_et:\",\n",
    "                dice_et,\n",
    "                \", time {:.2f}s\".format(time.time() - start_time),\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "    return run_acc.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.helper import save_checkpoint\n",
    "\n",
    "def trainer(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    loss_func,\n",
    "    acc_func,\n",
    "    scheduler,\n",
    "    model_inferer=None,\n",
    "    start_epoch=0,\n",
    "    post_sigmoid=None,\n",
    "    post_pred=None,\n",
    "):\n",
    "    val_acc_max = 0.0\n",
    "    dices_tc = []\n",
    "    dices_wt = []\n",
    "    dices_et = []\n",
    "    dices_avg = []\n",
    "    loss_epochs = []\n",
    "    trains_epoch = []\n",
    "    for epoch in range(start_epoch, max_epochs):\n",
    "        print(time.ctime(), \"Epoch:\", epoch)\n",
    "        epoch_time = time.time()\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            epoch=epoch,\n",
    "            loss_func=loss_func,\n",
    "        )\n",
    "        print(\n",
    "            \"Final training  {}/{}\".format(epoch, max_epochs - 1),\n",
    "            \"loss: {:.4f}\".format(train_loss),\n",
    "            \"time {:.2f}s\".format(time.time() - epoch_time),\n",
    "        )\n",
    "\n",
    "        if (epoch + 1) % val_every == 0 or epoch == 0:\n",
    "            loss_epochs.append(train_loss)\n",
    "            trains_epoch.append(int(epoch))\n",
    "            epoch_time = time.time()\n",
    "            val_acc = val_epoch(\n",
    "                model,\n",
    "                val_loader,\n",
    "                epoch=epoch,\n",
    "                acc_func=acc_func,\n",
    "                model_inferer=model_inferer,\n",
    "                post_sigmoid=post_sigmoid,\n",
    "                post_pred=post_pred,\n",
    "            )\n",
    "            dice_tc = val_acc[0]\n",
    "            dice_wt = val_acc[1]\n",
    "            dice_et = val_acc[2]\n",
    "            val_avg_acc = np.mean(val_acc)\n",
    "            print(\n",
    "                \"Final validation stats {}/{}\".format(epoch, max_epochs - 1),\n",
    "                \", dice_tc:\",\n",
    "                dice_tc,\n",
    "                \", dice_wt:\",\n",
    "                dice_wt,\n",
    "                \", dice_et:\",\n",
    "                dice_et,\n",
    "                \", Dice_Avg:\",\n",
    "                val_avg_acc,\n",
    "                \", time {:.2f}s\".format(time.time() - epoch_time),\n",
    "            )\n",
    "            dices_tc.append(dice_tc)\n",
    "            dices_wt.append(dice_wt)\n",
    "            dices_et.append(dice_et)\n",
    "            dices_avg.append(val_avg_acc)\n",
    "            if val_avg_acc > val_acc_max:\n",
    "                print(\"new best ({:.6f} --> {:.6f}). \".format(val_acc_max, val_avg_acc))\n",
    "                val_acc_max = val_avg_acc\n",
    "                save_checkpoint(\n",
    "                    model,\n",
    "                    epoch,\n",
    "                    best_acc=val_acc_max,\n",
    "                    dir_add=root_dir,\n",
    "                )\n",
    "            scheduler.step()\n",
    "    print(\"Training Finished !, Best Accuracy: \", val_acc_max)\n",
    "    return (\n",
    "        val_acc_max,\n",
    "        dices_tc,\n",
    "        dices_wt,\n",
    "        dices_et,\n",
    "        dices_avg,\n",
    "        loss_epochs,\n",
    "        trains_epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "(\n",
    "    val_acc_max,\n",
    "    dices_tc,\n",
    "    dices_wt,\n",
    "    dices_et,\n",
    "    dices_avg,\n",
    "    loss_epochs,\n",
    "    trains_epoch,\n",
    ") = trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=dice_loss,\n",
    "    acc_func=dice_acc,\n",
    "    scheduler=scheduler,\n",
    "    model_inferer=model_inferer,\n",
    "    start_epoch=start_epoch,\n",
    "    post_sigmoid=post_sigmoid,\n",
    "    post_pred=post_pred,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss and Dice metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, loss_epochs, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, dices_avg, color=\"green\")\n",
    "plt.show()\n",
    "plt.figure(\"train\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Val Mean Dice TC\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, dices_tc, color=\"blue\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice WT\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, dices_wt, color=\"brown\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice ET\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, dices_et, color=\"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test set dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_num = \"01619\"\n",
    "\n",
    "test_files = [\n",
    "    {\n",
    "        \"image\": [\n",
    "            os.path.join(\n",
    "                data_dir,\n",
    "                \"TrainingData/BraTS2021_\" + case_num + \"/BraTS2021_\" + case_num + \"_flair.nii.gz\",\n",
    "            ),\n",
    "            os.path.join(\n",
    "                data_dir,\n",
    "                \"TrainingData/BraTS2021_\" + case_num + \"/BraTS2021_\" + case_num + \"_t1ce.nii.gz\",\n",
    "            ),\n",
    "            os.path.join(\n",
    "                data_dir,\n",
    "                \"TrainingData/BraTS2021_\" + case_num + \"/BraTS2021_\" + case_num + \"_t1.nii.gz\",\n",
    "            ),\n",
    "            os.path.join(\n",
    "                data_dir,\n",
    "                \"TrainingData/BraTS2021_\" + case_num + \"/BraTS2021_\" + case_num + \"_t2.nii.gz\",\n",
    "            ),\n",
    "        ],\n",
    "        \"label\": os.path.join(\n",
    "            data_dir,\n",
    "            \"TrainingData/BraTS2021_\" + case_num + \"/BraTS2021_\" + case_num + \"_seg.nii.gz\",\n",
    "        ),\n",
    "    }\n",
    "]\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_ds = data.Dataset(data=test_files, transform=test_transform)\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best saved checkpoint and perform inference \n",
    "\n",
    "We select a single case from the validation set and perform inference to compare the model segmentation output with the corresponding label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"model.pt\"))[\"state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model_inferer_test = partial(\n",
    "    sliding_window_inference,\n",
    "    roi_size=[roi[0], roi[1], roi[2]],\n",
    "    sw_batch_size=1,\n",
    "    predictor=model,\n",
    "    overlap=0.6,\n",
    ")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data in test_loader:\n",
    "        image = batch_data[\"image\"].cuda()\n",
    "        prob = torch.sigmoid(model_inferer_test(image))\n",
    "        seg = prob[0].detach().cpu().numpy()\n",
    "        seg = (seg > 0.5).astype(np.int8)\n",
    "        seg_out = np.zeros((seg.shape[1], seg.shape[2], seg.shape[3]))\n",
    "        seg_out[seg[1] == 1] = 2\n",
    "        seg_out[seg[0] == 1] = 1\n",
    "        seg_out[seg[2] == 1] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize segmentation output and compare with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_num = 67\n",
    "img_add = os.path.join(\n",
    "    data_dir,\n",
    "    \"TrainingData/BraTS2021_\" + case_num + \"/BraTS2021_\" + case_num + \"_t1ce.nii.gz\",\n",
    ")\n",
    "label_add = os.path.join(\n",
    "    data_dir,\n",
    "    \"TrainingData/BraTS2021_\" + case_num + \"/BraTS2021_\" + case_num + \"_seg.nii.gz\",\n",
    ")\n",
    "img = nib.load(img_add).get_fdata()\n",
    "label = nib.load(label_add).get_fdata()\n",
    "plt.figure(\"image\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(img[:, :, slice_num], cmap=\"gray\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:, :, slice_num])\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"segmentation\")\n",
    "plt.imshow(seg_out[:, :, slice_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiorgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
